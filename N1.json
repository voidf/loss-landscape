{"model1": "tn10/vgg9_sgd_lr=0.1_bs=128_wd=0.0005_mom=0.9_save_epoch=1/model_300.t7", "model2": "tn09/vgg9_sgd_lr=0.1_bs=128_wd=0.0005_mom=0.9_save_epoch=1/model_300.t7", "method": "neb"}
{"model1": "tn10/vgg9_sgd_lr=0.1_bs=128_wd=0.0005_mom=0.9_save_epoch=1/model_300.t7", "model2": "tn09/vgg9_sgd_lr=0.1_bs=128_wd=0.0005_mom=0.9_save_epoch=1/model_300.t7", "method": "neb"}
{"model1": "R56_01/resnet56_sgd_lr=0.1_bs=128_wd=0.0005_mom=0.9_save_epoch=1/model_300.t7", "model2": "R56_02/resnet56_sgd_lr=0.1_bs=128_wd=0.0005_mom=0.9_save_epoch=1/model_300.t7", "method": "neb"}
step 0, loss: 2.4418859481811523
step 0, loss: 2.410024642944336
step 0, loss: 2.4622411727905273
step 0, loss: 2.4552299976348877
step 0, loss: 2.416292428970337
step 0, loss: 2.4775924682617188
step 2, loss: 2.185375213623047
step 12, loss: 1.9483925104141235
step 39, loss: 1.7378506660461426
step 97, loss: 1.5338810682296753
step 0, loss: 2.4011688232421875
step 4, loss: 2.0303993225097656
step 27, loss: 1.7988989353179932
step 90, loss: 1.609999179840088
step 123, loss: 1.4466146230697632
step 193, loss: 1.301250696182251
step 289, loss: 1.1514919996261597
step 351, loss: 1.0323671102523804
step 460, loss: 0.9053356647491455
step 647, loss: 0.8140150308609009
step 766, loss: 0.7140674591064453
step 1091, loss: 0.6343587040901184
step 1294, loss: 0.5681657195091248
step 1624, loss: 0.5099033713340759
step 1902, loss: 0.43167996406555176
step 2488, loss: 0.3818066716194153
step 2844, loss: 0.3411324620246887
step 2903, loss: 0.28981706500053406
step 3635, loss: 0.25954240560531616
step 4973, loss: 0.2321048229932785
step 5705, loss: 0.20806556940078735
step 6068, loss: 0.18346044421195984
step 6421, loss: 0.16418704390525818
step 6640, loss: 0.14483116567134857
step 7762, loss: 0.1291109323501587
step 8874, loss: 0.10690580308437347
step 9827, loss: 0.09526030719280243
step 10992, loss: 0.08375781774520874
step 12286, loss: 0.06789586693048477
step 15354, loss: 0.05798150599002838
step 15638, loss: 0.044690441340208054
step 20540, loss: 0.03738173097372055
step 22693, loss: 0.030900293961167336
step 26052, loss: 0.024779578670859337
step 27163, loss: 0.016678521409630775
step 0, loss: 2.3783857822418213
step 0, loss: 2.4061591625213623
step 0, loss: 2.4047811031341553
step 33, loss: 2.141542911529541
step 65, loss: 1.9170950651168823
step 130, loss: 1.688820719718933
step 0, loss: 2.454932689666748
step 0, loss: 2.42948317527771
step 31, loss: 2.1583213806152344
step 0, loss: 2.464883327484131
step 18, loss: 2.1969730854034424
step 55, loss: 1.9662895202636719
step 90, loss: 1.7330405712127686
step 219, loss: 1.5346589088439941
step 402, loss: 1.3649834394454956
step 537, loss: 1.2083698511123657
step 0, loss: 2.3770956993103027
step 0, loss: 2.440293788909912
step 0, loss: 2.4404454231262207
step 3, loss: 2.1836190223693848
step 8, loss: 1.869319200515747
step 41, loss: 1.6086169481277466
step 90, loss: 1.4126405715942383
step 149, loss: 1.2302300930023193
step 223, loss: 1.0609873533248901
step 0, loss: 2.4621105194091797
step 3, loss: 2.1454033851623535
step 11, loss: 1.8989204168319702
step 46, loss: 1.6791528463363647
step 104, loss: 1.5074787139892578
step 171, loss: 1.341597557067871
step 260, loss: 1.2053278684616089
step 354, loss: 1.0499836206436157
step 439, loss: 0.9321180582046509
step 602, loss: 0.838374674320221
step 784, loss: 0.7437480092048645
step 1023, loss: 0.6620101928710938
step 1379, loss: 0.5598531365394592
step 1931, loss: 0.4624577462673187
step 2508, loss: 0.412784218788147
step 2963, loss: 0.3664316236972809
step 3463, loss: 0.32620471715927124
